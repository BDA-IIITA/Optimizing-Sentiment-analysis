{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":8735258,"sourceType":"datasetVersion","datasetId":5243691}],"dockerImageVersionId":30734,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install --upgrade pip\n!pip install tweepy\n%pip install nltk\n%pip install plotly\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport tweepy as tw\n\nimport re\nimport nltk\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import *\nstopWords_nltk = set(stopwords.words('english'))\n\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\n\npd.options.plotting.backend = \"plotly\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = pd.read_csv('/kaggle/input/twitter-4datasets/Twitter_Data.csv')\ndf1\nprint(df1.shape)\n\ndf2 = pd.read_csv('/kaggle/input/twitter-4datasets/apple-twitter-sentiment-texts.csv')\ndf2 = df2.rename(columns={'text': 'clean_text', 'sentiment':'category'})\ndf2['category'] = df2['category'].map({-1: -1.0, 0: 0.0, 1:1.0})\n\ndf2\nprint(df2.shape)\n\ndf3 = pd.read_csv('/kaggle/input/twitter-4datasets/finalSentimentdata2.csv')\ndf3 = df3.rename(columns={'text': 'clean_text', 'sentiment':'category'})\ndf3['category'] = df3['category'].map({'sad': -1.0, 'anger': -1.0, 'fear': -1.0, 'joy':1.0})\ndf3 = df3.drop(['Unnamed: 0'], axis=1)\ndf3\nprint(df3.shape)\n\ndf4 = pd.read_csv('/kaggle/input/twitter-4datasets/Tweets.csv')\ndf4 = df4.rename(columns={'text': 'clean_text', 'airline_sentiment':'category'})\ndf4['category'] = df4['category'].map({'negative': -1.0, 'neutral': 0.0, 'positive':1.0})\ndf4 = df4[['category','clean_text']]\ndf4\nprint(df4.shape)\n\ndf = pd.concat([df1, df2, df3, df4], ignore_index=True)\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dropna(axis=0, inplace=True)\ndf.shape\ndf['category'] = df['category'].map({-1.0:'Negative', 0.0:'Neutral', 1.0:'Positive'})\ndf\nprint(df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"label\"] = df[\"category\"].map({\"Negative\" : 0, \"Neutral\" : 1, \"Positive\" : 2})\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\nfrom typing import Union, List\n\nclass CleanText():\n    \"\"\" clearing text except digits () . , word character \"\"\"\n\n    def __init__(self, clean_pattern = r\"[^A-ZĞÜŞİÖÇIa-zğüı'şöç0-9.\\\"',()]\"):\n        self.clean_pattern =clean_pattern\n\n    def __call__(self, text: Union[str, list]) -> List[List[str]]:\n\n        if isinstance(text, str):\n            docs = [[text]]\n\n        if isinstance(text, list):\n            docs = text\n\n        text = [[re.sub(self.clean_pattern, \" \", sent) for sent in sents] for sents in docs]\n\n        return text\n\ndef remove_emoji(data):\n    emoj = re.compile(\"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U00002500-\\U00002BEF\"\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        u\"\\U0001f926-\\U0001f937\"\n        u\"\\U00010000-\\U0010ffff\"\n        u\"\\u2640-\\u2642\"\n        u\"\\u2600-\\u2B55\"\n        u\"\\u200d\"\n        u\"\\u23cf\"\n        u\"\\u23e9\"\n        u\"\\u231a\"\n        u\"\\ufe0f\"  # dingbats\n        u\"\\u3030\"\n                      \"]+\", re.UNICODE)\n    return re.sub(emoj, '', data)\n\ndef tokenize(text):\n    \"\"\" basic tokenize method with word character, non word character and digits \"\"\"\n    text = re.sub(r\" +\", \" \", str(text))\n    text = re.split(r\"(\\d+|[a-zA-ZğüşıöçĞÜŞİÖÇ]+|\\W)\", text)\n    text = list(filter(lambda x: x != '' and x != ' ', text))\n    sent_tokenized = ' '.join(text)\n    return sent_tokenized\n\nregex = re.compile('[%s]' % re.escape(string.punctuation))\n\ndef remove_punct(text):\n    text = regex.sub(\" \", text)\n    return text\n\nclean = CleanText()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"tokenized_review\"] = df.clean_text.apply(lambda x: tokenize(x))\n# calculate token count for any sent\ndf[\"sent_token_length\"] = df[\"tokenized_review\"].apply(lambda x: len(x.split()))\n\nfig = px.histogram(df, x=\"sent_token_length\", nbins=20, color_discrete_sequence=px.colors.cmocean.algae, barmode='group', histnorm=\"percent\")\nfig.show()\n(df.sent_token_length < 50).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n                                          do_lower_case=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"sent_bert_token_length\"] = df[\"clean_text\"].apply(lambda x: len(tokenizer(x, add_special_tokens=False)[\"input_ids\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(df, x=\"sent_token_length\", nbins=20, color_discrete_sequence=px.colors.cmocean.algae, barmode='group', histnorm=\"percent\")\nfig.show()\n(df.sent_bert_token_length < 50).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport random\nfrom pathlib import Path\nimport json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom tqdm.notebook import tqdm\n\nfrom transformers import BertTokenizer\nfrom torch.utils.data import TensorDataset\n\nfrom transformers import BertForSequenceClassification","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config():\n    seed_val = 17\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    epochs = 5\n    batch_size = 6\n    seq_length = 512\n    lr = 2e-5\n    eps = 1e-8\n    pretrained_model = 'bert-base-uncased'\n    test_size=0.15\n    random_state=42\n    add_special_tokens=True\n    return_attention_mask=True\n    pad_to_max_length=True\n    do_lower_case=False\n    return_tensors='pt'\n\nconfig = Config()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\"seed_val\": config.seed_val,\n    \"device\":str(config.device),\n    \"epochs\":config.epochs,\n    \"batch_size\":config.batch_size,\n    \"seq_length\":config.seq_length,\n    \"lr\":config.lr,\n    \"eps\":config.eps,\n    \"pretrained_model\": config.pretrained_model,\n    \"test_size\":config.test_size,\n    \"random_state\":config.random_state,\n    \"add_special_tokens\":config.add_special_tokens,\n    \"return_attention_mask\":config.return_attention_mask,\n    \"pad_to_max_length\":config.pad_to_max_length,\n    \"do_lower_case\":config.do_lower_case,\n    \"return_tensors\":config.return_tensors,\n         }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\ndevice = config.device\n\nrandom.seed(config.seed_val)\nnp.random.seed(config.seed_val)\ntorch.manual_seed(config.seed_val)\ntorch.cuda.manual_seed_all(config.seed_val)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df_, val_df = train_test_split(df,\n                                    test_size=0.10,\n                                    random_state=config.random_state,\n                            stratify=df.label.values)\ntrain_df_.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, test_df = train_test_split(train_df_,\n                                    test_size=0.10,\n                                    random_state=42,\n                            stratify=train_df_.label.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(config.pretrained_model,\n                                          do_lower_case=config.do_lower_case)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_data_train = tokenizer.batch_encode_plus(\n    train_df.clean_text.values,\n    add_special_tokens=config.add_special_tokens,\n    return_attention_mask=config.return_attention_mask,\n    pad_to_max_length=config.pad_to_max_length,\n    max_length=config.seq_length,\n    return_tensors=config.return_tensors\n)\nencoded_data_val = tokenizer.batch_encode_plus(\n    val_df.clean_text.values,\n    add_special_tokens=config.add_special_tokens,\n    return_attention_mask=config.return_attention_mask,\n    pad_to_max_length=config.pad_to_max_length,\n    max_length=config.seq_length,\n    return_tensors=config.return_tensors\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids_train = encoded_data_train['input_ids']\nattention_masks_train = encoded_data_train['attention_mask']\nlabels_train = torch.tensor(train_df.label.values)\n\ninput_ids_val = encoded_data_val['input_ids']\nattention_masks_val = encoded_data_val['attention_mask']\nlabels_val = torch.tensor(val_df.label.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\ndataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(config.pretrained_model,\n                                                      num_labels=3,\n                                                      output_attentions=False,\n                                                      output_hidden_states=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\ndataloader_train = DataLoader(dataset_train,\n                              sampler=RandomSampler(dataset_train),\n                              batch_size=config.batch_size)\n\ndataloader_validation = DataLoader(dataset_val,\n                                   sampler=SequentialSampler(dataset_val),\n                                   batch_size=config.batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\n\noptimizer = AdamW(model.parameters(),\n                  lr=config.lr,\n                  eps=config.eps)\n\n\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                            num_warmup_steps=0,\n                                            num_training_steps=len(dataloader_train)*config.epochs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\ndef f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(labels_flat, preds_flat, average='weighted')\n\ndef accuracy_per_class(preds, labels, label_dict):\n    label_dict_inverse = {v: k for k, v in label_dict.items()}\n\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n\n    for label in np.unique(labels_flat):\n        y_preds = preds_flat[labels_flat==label]\n        y_true = labels_flat[labels_flat==label]\n        print(f'Class: {label_dict_inverse[label]}')\n        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(dataloader_val):\n\n    model.eval()\n\n    loss_val_total = 0\n    predictions, true_vals = [], []\n\n    for batch in dataloader_val:\n\n        batch = tuple(b.to(config.device) for b in batch)\n\n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n\n    # calculate avareage val loss\n    loss_val_avg = loss_val_total/len(dataloader_val)\n\n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n\n    return loss_val_avg, predictions, true_vals\n\nconfig.device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ntorch.cuda.empty_cache()\n\n\nmodel.to(config.device)\n\nfor epoch in tqdm(range(1, config.epochs+1)):\n\n    model.train()\n\n    loss_train_total = 0\n    # allows you to see the progress of the training\n    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n\n    for batch in progress_bar:\n\n        model.zero_grad()\n\n        batch = tuple(b.to(config.device) for b in batch)\n\n\n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }\n\n        outputs = model(**inputs)\n\n        loss = outputs[0]\n        loss_train_total += loss.item()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n        scheduler.step()\n\n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n\n\n    torch.save(model.state_dict(), f'_BERT_epoch_{epoch}.model')\n\n    tqdm.write(f'\\nEpoch {epoch}')\n\n    loss_train_avg = loss_train_total/len(dataloader_train)\n    tqdm.write(f'Training loss: {loss_train_avg}')\n\n    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n    val_f1 = f1_score_func(predictions, true_vals)\n    tqdm.write(f'Validation loss: {val_loss}')\n\n    tqdm.write(f'F1 Score (Weighted): {val_f1}');\n# save model params and other configs\nwith Path('params.json').open(\"w\") as f:\n    json.dump(params, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load(f'./_BERT_epoch_3.model', map_location=torch.device('cpu')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\npreds_flat = np.argmax(predictions, axis=1).flatten()\nprint(classification_report(preds_flat, true_vals))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}